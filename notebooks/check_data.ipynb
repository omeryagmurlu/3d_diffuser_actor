{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"Debugging...\")\n",
    "def custom_repr(self):\n",
    "    return f'{{Tensor:{tuple(self.shape)}}}'\n",
    "    # return f'{{Tensor:{tuple(self.shape)}}} {original_repr(self)}'\n",
    "\n",
    "original_repr = torch.Tensor.__repr__\n",
    "torch.Tensor.__repr__ = custom_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Nov 28 2023 23:51:11\n"
     ]
    }
   ],
   "source": [
    "from main_trajectory import traj_collate_fn, fig_to_numpy, Arguments\n",
    "from datasets.dataset_calvin import CalvinDataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters from the provided configuration\n",
    "dense_interpolation = 1\n",
    "interpolation_length = 20\n",
    "relative_action = 1\n",
    "\n",
    "image_rescale = \"0.75,1.25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't find dataset folder ../data/calvin/packaged_debug/training/A+0\n",
      "Can't find dataset folder ../data/calvin/packaged_debug/training/B+0\n",
      "Can't find dataset folder ../data/calvin/packaged_debug/training/C+0\n",
      "Created dataset from ../data/calvin/packaged_debug/training with 9\n",
      "Train dataset initialized\n"
     ]
    }
   ],
   "source": [
    "def load_instructions(instructions, split):\n",
    "    instructions = pickle.load(\n",
    "        open(f\"{instructions}/{split}.pkl\", \"rb\")\n",
    "    )['embeddings']\n",
    "    return instructions\n",
    "\n",
    "# Instruction paths\n",
    "train_instruction = load_instructions(\n",
    "    \"../instructions/calvin_task_debug_D\", 'training'\n",
    ")\n",
    "test_instruction = load_instructions(\n",
    "    \"../instructions/calvin_task_debug_D\", 'validation'\n",
    ")\n",
    "\n",
    "taskvar = [\n",
    "    (\"A\", 0), (\"B\", 0), (\"C\", 0), (\"D\", 0),\n",
    "]\n",
    "\n",
    "# Initialize datasets with specified arguments\n",
    "def get_train_dataset():\n",
    "    return CalvinDataset(\n",
    "        root=\"../data/calvin/packaged_debug/training\",\n",
    "        instructions=train_instruction,\n",
    "        taskvar=taskvar,\n",
    "        max_episode_length=30,\n",
    "        cache_size=0,\n",
    "        max_episodes_per_task=-1,\n",
    "        num_iters=600000,\n",
    "        cameras=[\"front\", \"wrist\"],\n",
    "        training=True,\n",
    "        image_rescale=tuple(float(x) for x in image_rescale.split(\",\")),\n",
    "        return_low_lvl_trajectory=True,\n",
    "        dense_interpolation=bool(dense_interpolation),\n",
    "        interpolation_length=interpolation_length,\n",
    "        relative_action=bool(relative_action)\n",
    "    )\n",
    "train_dataset = get_train_dataset()\n",
    "\n",
    "print(\"Train dataset initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['task', 'rgbs', 'pcds', 'action', 'instr', 'curr_gripper', 'curr_gripper_history', 'annotation_id', 'trajectory', 'trajectory_mask'])\n",
      "{'task': ['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D'], 'rgbs': {Tensor:(10, 2, 3, 160, 160)}, 'pcds': {Tensor:(10, 2, 3, 160, 160)}, 'action': {Tensor:(10, 8)}, 'instr': {Tensor:(10, 16, 512)}, 'curr_gripper': {Tensor:(10, 8)}, 'curr_gripper_history': {Tensor:(10, 3, 8)}, 'annotation_id': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'trajectory': {Tensor:(10, 20, 8)}, 'trajectory_mask': {Tensor:(10, 20)}}\n"
     ]
    }
   ],
   "source": [
    "for episode in train_dataset:\n",
    "    print(episode.keys())\n",
    "    # vis sometime later, for now:\n",
    "    print(episode)\n",
    "    # 10 is timestep, frames in the episode\n",
    "    # - 2 is multiview cam, 3 is rgb, 160x160\n",
    "    # - 8 is [x, y, z], [convert_rotation (quat, but dunno xyzw or wxyz)], [open]\n",
    "    # - - but calvin expects [x, y, z], [euler_x, euler_y, euler_z], [open]\n",
    "    # - 16 is, uhh dunno, but 512 is the size of the embedding\n",
    "    # - gripper is prolly 7 joint + gripper open/close?? doesnt make sense oherwise\n",
    "    # - traj is next max 20 actions for the timestep\n",
    "\n",
    "    # {\n",
    "    #     'task': [\n",
    "    #         'D', 'D', 'D', 'D', 'D',\n",
    "    #         'D', 'D', 'D', 'D', 'D'\n",
    "    #     ],\n",
    "    #     'rgbs': {Tensor: (10, 2, 3, 160, 160)},\n",
    "    #     'pcds': {Tensor: (10, 2, 3, 160, 160)},\n",
    "    #     'action': {Tensor: (10, 8)},\n",
    "    #     'instr': {Tensor: (10, 16, 512)},\n",
    "    #     'curr_gripper': {Tensor: (10, 8)},\n",
    "    #     'curr_gripper_history': {Tensor: (10, 3, 8)},\n",
    "    #     'trajectory': {Tensor: (10, 20, 8)},\n",
    "    #     'trajectory_mask': {Tensor: (10, 20)}\n",
    "    # }\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segregate 2d, 3d?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader, collation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/geekculture/pytorch-datasets-dataloader-samplers-and-the-collat-fn-bbfc7c527cf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Tensor:(2,)}, {Tensor:(2,)}, {Tensor:(2,)}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_collate(([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Tensor:(3,)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_collate([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Tensor:(2,)}, {Tensor:(2,)}, {Tensor:(2,)}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_collate([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Tensor:(2,)}, {Tensor:(2,)}, {Tensor:(2,)}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_collate([(1,2,3), (4,5,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_trajectory import traj_collate_fn\n",
    "# def traj_collate_fn(batch):\n",
    "#     keys = [\n",
    "#         \"trajectory\", \"trajectory_mask\",\n",
    "#         \"rgbs\", \"pcds\",\n",
    "#         \"curr_gripper\", \"curr_gripper_history\", \"action\", \"instr\"\n",
    "#     ]\n",
    "#     ret_dict = {\n",
    "#         key: torch.cat([\n",
    "#             item[key].float() if key != 'trajectory_mask' else item[key]\n",
    "#             for item in batch\n",
    "#         ]) for key in keys\n",
    "#     }\n",
    "\n",
    "#     ret_dict[\"task\"] = []\n",
    "#     ret_dict[\"annotation_id\"] = []\n",
    "#     for item in batch:\n",
    "#         ret_dict[\"task\"] += item['task']\n",
    "#         ret_dict[\"annotation_id\"] += item['annotation_id']\n",
    "#     return ret_dict\n",
    "collate_fn = traj_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't find dataset folder ../data/calvin/packaged_debug/training/A+0\n",
      "Can't find dataset folder ../data/calvin/packaged_debug/training/B+0\n",
      "Can't find dataset folder ../data/calvin/packaged_debug/training/C+0\n",
      "Created dataset from ../data/calvin/packaged_debug/training with 9\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "# train_sampler = DistributedSampler(train_dataset)\n",
    "train_loader = DataLoader(\n",
    "    get_train_dataset(),\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=0, # it's 4 in the code, but lets run this in the main thread for debugging the collate_fn\n",
    "    worker_init_fn=seed_worker,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    # sampler=train_sampler, # dont care about sampling really\n",
    "    drop_last=True,\n",
    "    generator=g\n",
    ")\n",
    "iter_loader = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['trajectory', 'trajectory_mask', 'rgbs', 'pcds', 'curr_gripper', 'curr_gripper_history', 'action', 'instr', 'task', 'annotation_id'])\n",
      "{'action': {Tensor:(24, 8)},\n",
      " 'annotation_id': {Tensor:(24,)},\n",
      " 'curr_gripper': {Tensor:(24, 8)},\n",
      " 'curr_gripper_history': {Tensor:(24, 3, 8)},\n",
      " 'instr': {Tensor:(24, 16, 512)},\n",
      " 'pcds': {Tensor:(24, 2, 3, 160, 160)},\n",
      " 'rgbs': {Tensor:(24, 2, 3, 160, 160)},\n",
      " 'task': ['D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D',\n",
      "          'D'],\n",
      " 'trajectory': {Tensor:(24, 20, 8)},\n",
      " 'trajectory_mask': {Tensor:(24, 20)}}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter_loader)\n",
    "print(batch.keys())\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'notebooks.visualizer'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [12/Oct/2024 22:49:08] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "from notebooks.visualizer import visualize_pointcloud\n",
    "import torch\n",
    "\n",
    "make2d = 2\n",
    "\n",
    "def twoderp(pcds):\n",
    "    pcds = pcds.clone()\n",
    "    pcds[0, [2]] = 1\n",
    "    pcds[1, [2]] = 0\n",
    "    pcds[:, [0]], pcds[:, [1]] = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, pcds.shape[-2]),\n",
    "        torch.linspace(1, -1, pcds.shape[-1]),\n",
    "        indexing='xy'\n",
    "    )\n",
    "    return pcds\n",
    "\n",
    "colors = batch['rgbs'][0] * 255\n",
    "\n",
    "if make2d == 1:\n",
    "    pcds = twoderp(batch['pcds'][0])\n",
    "elif make2d == 2:\n",
    "    pcds = batch['pcds'][0]\n",
    "    tdpcds = twoderp(pcds)\n",
    "    pcds = torch.cat([pcds, tdpcds], dim=0)\n",
    "    colors = torch.cat([colors, colors], dim=0)\n",
    "else:\n",
    "    pcds = batch['pcds'][0]\n",
    "\n",
    "visualize_pointcloud(einops.rearrange([pcds, colors], 'p n c h w -> (n h w) (p c)'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
